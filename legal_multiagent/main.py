from __future__ import annotations

from langgraph.graph import StateGraph

from typing import TypedDict, Optional
from agents.extract_basic_facts import extract_basic_facts
from agents.classify_legal_domains import classify_legal_domains
from agents.generate_legal_issue import generate_legal_issue
from agents.generate_precedent_queries import generate_precedent_queries
from agents.search_similar_precedents_from_supabase import search_similar_precedents_from_supabase
from agents.recommend_relevant_laws import recommend_relevant_laws
from agents.simulate_judgment import simulate_judgment
from agents.predict_sentence import predict_sentence

# Langfuse CallbackHandler for Langgraph

#openai
from openai import OpenAI
from pydantic import BaseModel
import os
from dotenv import load_dotenv
import json

load_dotenv(override=True)

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

#openai
client = OpenAI(api_key = OPENAI_API_KEY)

class LegalCaseState(TypedDict):
    # â”€â”€â”€â”€â”€ ì½ê¸° ì „ìš© â”€â”€â”€â”€â”€
    user_input: str                       # ìµœì´ˆ ì‚¬ìš©ì ì§ˆë¬¸
    case_type: Optional[str]            # ì‚¬ê±´ ìœ í˜• (ë¯¼ì‚¬/í˜•ì‚¬/í–‰ì •)

    # â”€â”€â”€â”€â”€ ê° ë…¸ë“œì—ì„œ ì—…ë°ì´íŠ¸í•  í•„ë“œ â”€â”€â”€â”€â”€
    legal_issue: Optional[str]            # âœ¦ ì´ìŠˆ ì¶”ì¶œ ë…¸ë“œ
    precedent_queries: Optional[list[str]]# âœ¦ íŒë¡€ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± ë…¸ë“œ
    precedent_summary: Optional[str]      # âœ¦ íŒë¡€ ìš”ì•½ ë…¸ë“œ
    law_recommendation: Optional[str]     # âœ¦ ê´€ë ¨ ì¡°ë¬¸ ì¶”ì²œ ë…¸ë“œ
    legal_judgment_prediction: Optional[str]  # âœ¦ íŒê²° ê²°ê³¼ ì˜ˆì¸¡ ë…¸ë“œ
    sentence_prediction: Optional[str]    # âœ¦ í˜•ëŸ‰ ì˜ˆì¸¡ ë…¸ë“œ
    basic_facts: Optional[list[str]]      # âœ¦ ì‚¬ì‹¤ê´€ê³„ ì¶”ì¶œ ë…¸ë“œ
    case_categories: Optional[list[str]]  # âœ¦ ì‚¬ê±´ ë¶„ë¥˜ ë…¸ë“œ
    precedent_cases: Optional[list[str]]  # âœ¦ ì‚¬ê±´ë²ˆí˜¸/ì‚¬ê±´ëª… ì¶”ì¶œ ë…¸ë“œ
    final_answer: Optional[str]           # âœ¦ ìµœì¢… ë‹µë³€ ì¡°ë¦½ ë…¸ë“œ

workflow = StateGraph(LegalCaseState)

def classify_legal_domains_node(state: LegalCaseState) -> LegalCaseState:
    print("â–¶ï¸ ì‚¬ê±´ ë¶„ì•¼ ë¶„ë¥˜ ì¤‘...")
    categories = classify_legal_domains(state["user_input"])
    return {"case_categories": categories}                 # âœ…

def extract_basic_facts_node(state: LegalCaseState) -> LegalCaseState:
    print("â–¶ï¸ ì‚¬ì‹¤ê´€ê³„ ì¶”ì¶œ ì¤‘...")
    facts = extract_basic_facts(state["user_input"])
    return {"basic_facts": facts}                          # âœ…

def extract_legal_issue_node(state: LegalCaseState) -> LegalCaseState:
    print("â–¶ï¸ ë²•ì  ìŸì  ì¶”ì¶œ ì¤‘...")
    issue = generate_legal_issue(state["user_input"])
    return {"legal_issue": issue}                          # âœ…

def generate_precedent_queries_node(state: LegalCaseState) -> LegalCaseState:
    print("â–¶ï¸ íŒë¡€ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± ì¤‘...")
    queries = generate_precedent_queries(
        legal_issue=state["legal_issue"],
        basic_facts=state["basic_facts"],
        case_categories=state["case_categories"]
    )
    return {"precedent_queries": queries}                  # âœ…

def summarize_precedents_node(state: LegalCaseState) -> LegalCaseState:
    print("â–¶ï¸ ìœ ì‚¬ íŒë¡€ ìš”ì•½ ì¤‘...")
    summary, matches = search_similar_precedents_from_supabase(
        precedent_queries=state["precedent_queries"],
        legal_issue=state["legal_issue"],
        basic_facts=state["basic_facts"],
        legal_domains=state["case_categories"]
    )

    # ğŸ†• ì‚¬ê±´ë²ˆí˜¸ + ì‚¬ê±´ëª… 10ê°œ ì¶”ì¶œ
    cases = [
        f"{m.get('caseno','')} / {m.get('casenm','')}"
        for m in matches[:10]
    ]
    return {
        "precedent_summary": summary,
        "precedent_matches": matches,
        "precedent_cases":   cases,   # <- ì¶”ê°€
    }

def recommend_law_node(state: LegalCaseState):
    print("â–¶ï¸ ê´€ë ¨ ë²•ë ¹ ì¶”ì²œ ì¤‘...")
    print("ğŸ§ª ì…ë ¥ ìƒíƒœ í™•ì¸:", state)
    laws_dict = recommend_relevant_laws(
        legal_issue     = state["legal_issue"],
        facts           = state["basic_facts"],
        case_categories = state["case_categories"]
    )
    print("ğŸ“¤ GPT ì‘ë‹µ ì›ë¬¸:", laws_dict)
    import json
    if isinstance(laws_dict, str):
        try:
            laws_dict = json.loads(laws_dict)
        except Exception as e:
            print("âŒ JSON íŒŒì‹± ì‹¤íŒ¨:", e)
            raise
    if isinstance(laws_dict, dict) and "laws" in laws_dict:
        laws = laws_dict["laws"]
    else:
        laws = laws_dict  # already a list
    return {"law_recommendation": laws}

def simulate_judgment_node(state: LegalCaseState) -> LegalCaseState:
    print("â–¶ï¸ íŒê²° ê²°ê³¼ ì˜ˆì¸¡ ì¤‘...")
    judgment_text = simulate_judgment(
        facts=state.get("basic_facts", []),
        precedents_summary=state.get("precedent_summary", ""),
        law_articles=state.get("law_recommendation", []),
        case_type=state.get("case_type", ""),
    ).strip()

    if not judgment_text:
        judgment_text = "íŒê²° ê²°ê³¼ ì˜ˆì¸¡ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

    return {"legal_judgment_prediction": judgment_text}

def predict_sentence_node(state: LegalCaseState) -> LegalCaseState:
    print("â–¶ï¸ í˜•ëŸ‰ ì˜ˆì¸¡ ì¤‘...")
    sentence = predict_sentence(
        facts=state["basic_facts"],
        law_articles=state["law_recommendation"],
        precedent_summary=state["precedent_summary"]
    )
    return {"sentence_prediction": sentence}

def generate_final_answer_node(state: LegalCaseState) -> LegalCaseState:
    print("â–¶ï¸ ìµœì¢… ë‹µë³€ ì¡°ë¦½ ì¤‘...")
    parts = [
        f"\n\nâœ… ì‚¬ê±´ ë¶„ì•¼: {', '.join(state['case_categories'])}" if state.get("case_categories") else "",

        f"ğŸ” ê¸°ì´ˆ ì‚¬ì‹¤:\n" + "\n".join(state["basic_facts"])
        if state.get("basic_facts") else "",

        f"âš–ï¸ ë²•ì  ìŸì : {state['legal_issue']}"             if state.get("legal_issue") else "",

        f"ğŸ“– ì ìš© ë²•ë ¹:\n" + ("\n".join(state["law_recommendation"])
                             if isinstance(state["law_recommendation"], list)
                             else state["law_recommendation"])
        if state.get("law_recommendation") else "",

        f"ğŸ“ ì°¸ê³  íŒë¡€(ì‚¬ê±´ë²ˆí˜¸ / ì‚¬ê±´ëª…):\n" + "\n".join(state["precedent_cases"])
        if state.get("precedent_cases") else "",
        f"ğŸ“š ìœ ì‚¬ íŒë¡€: {state['precedent_summary']}"  if state.get("precedent_summary") else "",
        f"ğŸ§‘â€âš–ï¸ ì˜ˆìƒ íŒê²° ìš”ì§€: {state['legal_judgment_prediction'].strip()}" if state.get("legal_judgment_prediction") and state["legal_judgment_prediction"].strip() else "",
    ]
    if state.get("sentence_prediction") and state["sentence_prediction"].strip():
        parts.append(f"ğŸ” ì˜ˆìƒ í˜•ëŸ‰: {state['sentence_prediction']}")
    final_output = "\n\n\n".join([p for p in parts if p])
    return {"final_answer": final_output}                   # âœ…

from langchain_core.runnables import RunnableLambda
from langfuse.callback import CallbackHandler as LangfuseCallbackHandler
import os

# Langfuse í•¸ë“¤ëŸ¬ ì„¤ì •
langfuse_handler = LangfuseCallbackHandler(
    public_key=os.getenv("LANGFUSE_PUBLIC_KEY"),
    secret_key=os.getenv("LANGFUSE_SECRET_KEY"),
    host=os.getenv("LANGFUSE_HOST", "https://cloud.langfuse.com"),
)

def create_workflow() -> StateGraph:
    workflow = StateGraph(LegalCaseState)

    # â”€â”€ 1ï¸âƒ£ ê³µí†µ ë…¸ë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    workflow.add_node("ExtractLegalIssue",  RunnableLambda(extract_legal_issue_node))
    workflow.add_node("ExtractBasicFacts",  RunnableLambda(extract_basic_facts_node))
    workflow.add_node("ClassifyCaseType",   RunnableLambda(classify_legal_domains_node))
    workflow.add_node("GenerateFinalAnswer", RunnableLambda(generate_final_answer_node))

    # â”€â”€ 2ï¸âƒ£ ë¸Œëœì¹˜ ë…¸ë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    branch_labels = ["ë¯¼ì‚¬", "í˜•ì‚¬", "í–‰ì •"]
    for label in branch_labels:
        gq   = f"GeneratePrecedentQuery_{label}"
        sum_ = f"SummarizePrecedents_{label}"
        law  = f"RecommendLaw_{label}"
        sim  = f"SimulateJudgment_{label}"
        sen  = f"PredictSentence_{label}"

        workflow.add_node(gq,  RunnableLambda(lambda s, l=label: generate_precedent_queries_node({**s, "case_type": l})))
        workflow.add_node(sum_, RunnableLambda(lambda s, l=label: summarize_precedents_node({**s, "case_type": l})))
        workflow.add_node(law, RunnableLambda(lambda s, l=label: recommend_law_node({**s, "case_type": l})))
        workflow.add_node(sim, RunnableLambda(lambda s, l=label: simulate_judgment_node({**s, "case_type": l})))
        if label == "í˜•ì‚¬":
            workflow.add_node(sen, RunnableLambda(lambda s, l=label: predict_sentence_node({**s, "case_type": l})))

        workflow.add_edge(gq,  sum_)
        workflow.add_edge(sum_, law)
        workflow.add_edge(law, sim)
        if label == "í˜•ì‚¬":
            workflow.add_edge(sim, sen)
            workflow.add_edge(sen, "GenerateFinalAnswer")
        else:
            workflow.add_edge(sim, "GenerateFinalAnswer")

    # â”€â”€ 3ï¸âƒ£ ì‚¬ê±´ ë¶„ë¥˜ ì¡°ê±´ë¶€ íë¦„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def case_router(state: LegalCaseState) -> str:
        cats = state.get("case_categories") or []
        for l in ("í˜•ì‚¬", "ë¯¼ì‚¬", "í–‰ì •"):
            if l in cats:
                return l
        return "ê¸°íƒ€"

    workflow.add_conditional_edges(
        "ClassifyCaseType",
        case_router,
        {
            "ë¯¼ì‚¬":  "GeneratePrecedentQuery_ë¯¼ì‚¬",
            "í˜•ì‚¬":  "GeneratePrecedentQuery_í˜•ì‚¬",
            "í–‰ì •":  "GeneratePrecedentQuery_í–‰ì •",
            "ê¸°íƒ€":  "GenerateFinalAnswer",
        }
    )

    # â”€â”€ 4ï¸âƒ£ ê¸°ë³¸ ì§ë ¬ ì—°ê²° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    workflow.set_entry_point("ExtractLegalIssue")
    workflow.add_edge("ExtractLegalIssue", "ExtractBasicFacts")
    workflow.add_edge("ExtractBasicFacts", "ClassifyCaseType")
    workflow.set_finish_point("GenerateFinalAnswer")

    # âœ… Langfuse íŠ¸ë˜í‚¹ ì ìš©
    return workflow.compile().with_config({
        "callbacks": [langfuse_handler]
    })

# ê·¸ë˜í”„ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
graph = create_workflow()

# ì´ˆê¸° ìƒíƒœ ì •ì˜ (LegalCaseState ê¸°ì¤€)
state = {
    "user_input": """
ì €ëŠ” 2022ë…„ 3ì›” 1ì¼ë¶€í„° ì„œìš¸ì‹œ ë§ˆí¬êµ¬ì— ìˆëŠ” ë‹¤ì„¸ëŒ€ì£¼íƒì˜ 2ì¸µì„ ì „ì„¸ë¡œ ì„ì°¨í•˜ì—¬ ê±°ì£¼í•´ ì™”ìŠµë‹ˆë‹¤.  
ê³„ì•½ ë‹¹ì‹œ ë³´ì¦ê¸ˆì€ 1ì–µ 5ì²œë§Œ ì›ì´ì—ˆê³ , ê³„ì•½ê¸°ê°„ì€ 2ë…„ìœ¼ë¡œ ì„¤ì •ë˜ì–´ ìˆì—ˆìœ¼ë©°, 2024ë…„ 2ì›” 29ì¼ì— ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.  
ì§‘ì£¼ì¸ê³¼ëŠ” í‘œì¤€ì„ëŒ€ì°¨ê³„ì•½ì„œë¥¼ ì‘ì„±í•˜ì˜€ê³ , ë³´ì¦ê¸ˆ ë°˜í™˜ê³¼ ê´€ë ¨í•˜ì—¬ íŠ¹ì•½ì‚¬í•­ì€ ë”°ë¡œ ëª…ì‹œí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.  
ê³„ì•½ ê°±ì‹ ì€ í•˜ì§€ ì•Šê¸°ë¡œ í•˜ê³ , ì €ëŠ” ê³„ì•½ ì¢…ë£Œì¼ì— ë§ì¶° ì´ì‚¬ë¥¼ ì¤€ë¹„í•˜ê³  ìƒˆë¡œìš´ ê±°ì²˜ë„ ë§ˆë ¨í•˜ì˜€ìŠµë‹ˆë‹¤.  

í•˜ì§€ë§Œ ì´ì‚¬ í•˜ë£¨ ì „ì¸ 2024ë…„ 2ì›” 28ì¼, ì§‘ì£¼ì¸ì´ ê°‘ìê¸° ë³´ì¦ê¸ˆì´ ë‹¹ì¥ ë§ˆë ¨ë˜ì§€ ì•Šì•˜ë‹¤ë©° ë°˜í™˜ì„ ë¯¸ë£° ìˆ˜ë°–ì— ì—†ë‹¤ê³  í†µë³´í•´ ì™”ìŠµë‹ˆë‹¤.  
ì´ì— ë”°ë¼ ì €ëŠ” ì¼ë‹¨ ìƒˆ ì§‘ìœ¼ë¡œ ì´ì‚¬ë¥¼ ì™„ë£Œí•œ í›„, ë³´ì¦ê¸ˆ ë°˜í™˜ì„ ìš”ì²­í•˜ëŠ” ë‚´ìš©ì¦ëª…ì„ ë³´ëƒˆì§€ë§Œ,  
ì§‘ì£¼ì¸ì€ ê³„ì†í•´ì„œ â€œìê¸ˆ ì‚¬ì •ì´ ì–´ë µë‹¤â€ëŠ” ì´ìœ ë¡œ ë°˜í™˜ì„ ë¯¸ë£¨ê³  ìˆìŠµë‹ˆë‹¤.  

í˜„ì¬ í•´ë‹¹ ì£¼íƒì—ëŠ” ìƒˆë¡œìš´ ì„¸ì…ìë„ ë“¤ì–´ì˜¤ì§€ ì•Šì€ ìƒíƒœì´ë©°,  
ì§‘ì£¼ì¸ì€ ì „ì„¸ë³´ì¦ê¸ˆì„ ë°˜í™˜í•  ê³„íšë„, ì¼ì •ë„ ëª…í™•í•˜ê²Œ ì œì‹œí•˜ì§€ ì•Šê³  ìˆìŠµë‹ˆë‹¤.  
ì €ëŠ” í•´ë‹¹ ë³´ì¦ê¸ˆìœ¼ë¡œ ìƒˆ ì§‘ ì „ì„¸ìê¸ˆì„ ì¶©ë‹¹í•´ì•¼ í•˜ëŠ” ìƒí™©ì´ì—ˆê¸°ì—  
í˜„ì¬ ì€í–‰ ëŒ€ì¶œì„ ë°›ì•„ ì´ì‚¬ë¹„ìš©ì„ ì¶©ë‹¹í•œ ìƒíƒœì´ë©°, ì´ë¡œ ì¸í•´ ê²½ì œì  ì†í•´ì™€ ì •ì‹ ì  ìŠ¤íŠ¸ë ˆìŠ¤ê°€ ìƒë‹¹í•©ë‹ˆë‹¤.  

ë˜í•œ ì§‘ì£¼ì¸ì€ ì—°ë½ì„ íšŒí”¼í•˜ê³  ìˆìœ¼ë©°, ì „í™”ë‚˜ ë©”ì‹œì§€ì—ë„ ì œëŒ€ë¡œ ì‘ë‹µí•˜ì§€ ì•Šê³  ìˆì–´ ìë ¥ìœ¼ë¡œ ë³´ì¦ê¸ˆì„ ëŒë ¤ë°›ê¸° ì–´ë ¤ìš´ ìƒí™©ì…ë‹ˆë‹¤.  
ì´ëŸ¬í•œ ìƒí™©ì—ì„œ ì œê°€ ì·¨í•  ìˆ˜ ìˆëŠ” ë²•ì  ì¡°ì¹˜ì—ëŠ” ì–´ë–¤ ê²ƒë“¤ì´ ìˆìœ¼ë©°,  
ì‹¤ì œë¡œ ì†Œì†¡ì„ ì§„í–‰í•˜ê²Œ ëœë‹¤ë©´ ì–´ë–¤ ì ˆì°¨ì™€ ì¦ê±°ê°€ í•„ìš”í•œì§€,  
ë³´ì¦ê¸ˆì„ ëŒë ¤ë°›ê¸°ê¹Œì§€ ì–¼ë§ˆë‚˜ ê±¸ë¦´ ìˆ˜ ìˆëŠ”ì§€ ë“± êµ¬ì²´ì ì¸ ì¡°ì–¸ì„ ë°›ê³  ì‹¶ìŠµë‹ˆë‹¤.
"""
}

import time
print("âœ… LangGraph ì‹¤í–‰ ì¤€ë¹„ ì™„ë£Œ")
start_time = time.time()

# LangGraph ì‹¤í–‰
result = graph.invoke(state)

# ìµœì¢… ë‹µë³€ ì¶œë ¥
print(result["final_answer"])
print("\n\nâ±ï¸ ì‹¤í–‰ ì‹œê°„: {:.2f}ì´ˆ".format(time.time() - start_time))