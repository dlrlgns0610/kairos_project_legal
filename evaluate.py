import os
import json
import random
import glob
import subprocess
import re
from typing import List, Dict, Tuple
from collections import defaultdict

# For evaluation metrics
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from rouge import Rouge
from bert_score import score as bert_score_score

# For OpenAI API
from openai import OpenAI
from dotenv import load_dotenv

# For Konlpy
from konlpy.tag import Okt

# Import general legal advisor
import sys
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from legal_multiagent.agents.general_legal_advisor import general_legal_advice

# Load environment variables
load_dotenv(override=True)
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=OPENAI_API_KEY)

# Assuming the root directory of the project is the current working directory
PROJECT_ROOT = os.getcwd()
PRECEDENT_DATA_DIR = os.path.join(PROJECT_ROOT, 'á„‘á…¡á†«á„…á…¨á„ƒá…¦á„‹á…µá„á…¥')
MAIN_APP_PATH = os.path.join(PROJECT_ROOT, 'legal_multiagent', 'main.py')

okt = Okt() # Initialize Konlpy Okt globally

def select_random_precedent_file() -> str:
    """
    íŒë¡€ ë°ì´í„° ë””ë ‰í† ë¦¬ì—ì„œ ë¬´ì‘ìœ„ë¡œ í•˜ë‚˜ì˜ JSON íŒŒì¼ ê²½ë¡œë¥¼ ì„ íƒí•©ë‹ˆë‹¤.
    """
    json_files = glob.glob(os.path.join(PRECEDENT_DATA_DIR, '**', '*.json'), recursive=True)
    if not json_files:
        raise FileNotFoundError(f"No JSON files found in {PRECEDENT_DATA_DIR}")
    return random.choice(json_files)

def load_precedent_data(file_path: str) -> Dict:
    """
    ì£¼ì–´ì§„ íŒë¡€ JSON íŒŒì¼ì—ì„œ í•„ìš”í•œ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    """
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    basic_facts = data.get('facts', {}).get('bsisFacts', [])
    court_dcss = data.get('dcss', {}).get('courtDcss', [])
    cnclsns = data.get('close', {}).get('cnclsns', [])
    
    # courtDcssì™€ cnclsnsë¥¼ ê²°í•©í•˜ì—¬ ìµœì¢… ê²°ë¡ ì˜ ì°¸ì¡° í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©
    reference_conclusion = " ".join(court_dcss + cnclsns)
    
    return {
        'basicFacts': basic_facts,
        'referenceConclusion': reference_conclusion
    }

def generate_client_request_text(basic_facts: List[str]) -> str:
    """
    ì¶”ì¶œëœ basicFactsë¥¼ ê¸°ë°˜ìœ¼ë¡œ OpenAI APIë¥¼ ì‚¬ìš©í•˜ì—¬ í´ë¼ì´ì–¸íŠ¸ ìš”ì²­ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    facts_str = "\n".join([f"{i+1}. {fact}" for i, fact in enumerate(basic_facts)])
    
    messages = [
        {
            "role": "system",
            "content": """
            ë‹¹ì‹ ì€ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ 'ê¸°ì´ˆ ì‚¬ì‹¤'ì„ ë°”íƒ•ìœ¼ë¡œ, í•´ë‹¹ ì‚¬ê±´ì— ëŒ€í•´ ì¼ë°˜ì¸ì´ ë³€í˜¸ì‚¬ì—ê²Œ ë¬¸ì˜í•  ë²•ë¥  ìƒë‹´ ìš”ì²­ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.
            ìš”ì²­ í…ìŠ¤íŠ¸ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë‚´ìš©ì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤:
            1. ìì‹ ì´ ì²˜í•œ ìƒí™©ì— ëŒ€í•œ ì„¤ëª… (ì£¼ì–´ì§„ ê¸°ì´ˆ ì‚¬ì‹¤ì„ ë°”íƒ•ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì„œìˆ )
            2. ê¶ê¸ˆí•œ ë²•ì  ì¡°ì¹˜, ì ˆì°¨, í•„ìš”í•œ ì¦ê±°, ì˜ˆìƒ ì†Œìš” ì‹œê°„ ë“± êµ¬ì²´ì ì¸ ì§ˆë¬¸
            3. ì‹¤ì œ ì‚¬ëŒì´ ì‘ì„±í•œ ê²ƒì²˜ëŸ¼ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì²´
            """
        },
        {
            "role": "user",
            "content": f"""
            ## ê¸°ì´ˆ ì‚¬ì‹¤
            {facts_str}

            ## ìƒì„±í•  ë²•ë¥  ìƒë‹´ ìš”ì²­ í…ìŠ¤íŠ¸ ì˜ˆì‹œ:
            ì €ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ìƒí™©ì— ì²˜í•´ ìˆìŠµë‹ˆë‹¤. [ê¸°ì´ˆ ì‚¬ì‹¤ì„ ë°”íƒ•ìœ¼ë¡œ ìƒí™© ì„¤ëª…].
            ì´ëŸ¬í•œ ìƒí™©ì—ì„œ ì œê°€ ì·¨í•  ìˆ˜ ìˆëŠ” ë²•ì  ì¡°ì¹˜ì—ëŠ” ì–´ë–¤ ê²ƒë“¤ì´ ìˆìœ¼ë©°,
            ì‹¤ì œë¡œ ì†Œì†¡ì„ ì§„í–‰í•˜ê²Œ ëœë‹¤ë©´ ì–´ë–¤ ì ˆì°¨ì™€ ì¦ê±°ê°€ í•„ìš”í•œì§€,
            ë³´ì¦ê¸ˆì„ ëŒë ¤ë°›ê¸°ê¹Œì§€ ì–¼ë§ˆë‚˜ ê±¸ë¦´ ìˆ˜ ìˆëŠ”ì§€ ë“± êµ¬ì²´ì ì¸ ì¡°ì–¸ì„ ë°›ê³  ì‹¶ìŠµë‹ˆë‹¤.
            """
        }
    ]

    try:
        response = client.chat.completions.create(
            model="gpt-4o",  # ë˜ëŠ” "gpt-3.5-turbo" ë“± ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸
            messages=messages,
            temperature=0.7, # ì°½ì˜ì„±ì„ ìœ„í•´ ì ì ˆí•œ ì˜¨ë„ ì„¤ì •
            max_tokens=500,
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"Error generating client request text with OpenAI API: {e}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ì¡´ í…œí”Œë¦¿ ì‚¬ìš©
        request_template = f"""\
ì €ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ìƒí™©ì— ì²˜í•´ ìˆìŠµë‹ˆë‹¤.\n\
{facts_str}\n\
\n\
ì´ëŸ¬í•œ ìƒí™©ì—ì„œ ì œê°€ ì·¨í•  ìˆ˜ ìˆëŠ” ë²•ì  ì¡°ì¹˜ì—ëŠ” ì–´ë–¤ ê²ƒë“¤ì´ ìˆìœ¼ë©°,\n\
ì‹¤ì œë¡œ ì†Œì†¡ì„ ì§„í–‰í•˜ê²Œ ëœë‹¤ë©´ ì–´ë–¤ ì ˆì°¨ì™€ ì¦ê±°ê°€ í•„ìš”í•œì§€,\n\
ë³´ì¦ê¸ˆì„ ëŒë ¤ë°›ê¸°ê¹Œì§€ ì–¼ë§ˆë‚˜ ê±¸ë¦´ ìˆ˜ ìˆëŠ”ì§€ ë“± êµ¬ì²´ì ì¸ ì¡°ì–¸ì„ ë°›ê³  ì‹¶ìŠµë‹ˆë‹¤.\n\
"""
        return request_template.strip()

def run_main_and_get_output(client_request_text: str) -> Dict:
    """
    main.pyë¥¼ ì‹¤í–‰í•˜ê³  ê·¸ ê²°ê³¼ë¥¼ íŒŒì‹±í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    main.py íŒŒì¼ì„ ì„ì‹œë¡œ ìˆ˜ì •í•˜ì—¬ client_request_textë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ë„ë¡ í•©ë‹ˆë‹¤.
    """
    original_main_content = ""
    with open(MAIN_APP_PATH, 'r', encoding='utf-8') as f:
        original_main_content = f.read()

    # main.py ìˆ˜ì •: user_inputì„ í•˜ë“œì½”ë”©
    # client_request_text ë‚´ì˜ ì‚¼ì¤‘ ë”°ì˜´í‘œë¥¼ ì„ì‹œë¡œ ëŒ€ì²´í•˜ì—¬ êµ¬ë¬¸ ì˜¤ë¥˜ ë°©ì§€
    escaped_client_request_text = client_request_text.replace('"""', '___TRIPLE_QUOTE___')
    modified_main_content = re.sub(
        r"user_input = sys\\.stdin\\.read\\(\\)",
        f"user_input = \"\"\"{escaped_client_request_text}\"\"\"",
        original_main_content
    )
    
    with open(MAIN_APP_PATH, 'w', encoding='utf-8') as f:
        f.write(modified_main_content)

    result = {}
    try:
        process = subprocess.run(
            ["python", "-m", "legal_multiagent.main"],
            capture_output=True,
            text=True,
            encoding='utf-8',
            check=True
        )
        # main.pyì˜ stdoutì—ì„œ JSON ê²°ê³¼ íŒŒì‹±
        result = json.loads(process.stdout)
    except subprocess.CalledProcessError as e:
        print(f"Error running main.py: {e}")
        print(f"Stdout: {e.stdout}")
        print(f"Stderr: {e.stderr}")
    except json.JSONDecodeError as e:
        print(f"JSON decode error: {e}")
        print(f"Raw stdout: {process.stdout}")
    finally:
        # main.pyë¥¼ ì›ë˜ ìƒíƒœë¡œ ë˜ëŒë¦¼
        with open(MAIN_APP_PATH, 'w', encoding='utf-8') as f:
            f.write(original_main_content)
    return result

def run_general_model_and_get_output(client_request_text: str) -> Dict:
    """
    general_legal_advisorë¥¼ ì§ì ‘ í˜¸ì¶œí•˜ì—¬ ê²°ê³¼ë¥¼ íŒŒì‹±í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    ì´ì œ general_legal_advisorëŠ” JSON í˜•ì‹ì˜ ë¬¸ìì—´ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        general_advice_json_str = general_legal_advice(user_input=client_request_text)
        # JSON ë¬¸ìì—´ì„ íŒŒì‹±í•˜ì—¬ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        advice_dict = json.loads(general_advice_json_str)
        return advice_dict
    except Exception as e:
        print(f"Error running general model or parsing JSON: {e}")
        return {"reconstructed_facts": "", "legal_conclusion": ""}

def extract_generated_texts(main_output: Dict, is_general_model: bool = False) -> Dict:
    """
    ëª¨ë¸ì˜ ì¶œë ¥ì—ì„œ ìƒì„±ëœ ì‚¬ì‹¤ê´€ê³„ì™€ ìµœì¢… ê²°ë¡  í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    is_general_model í”Œë˜ê·¸ë¥¼ ì¶”ê°€í•˜ì—¬ ì¼ë°˜ ëª¨ë¸ì˜ ì¶œë ¥ì„ ë‹¤ë¥´ê²Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    if is_general_model:
        # ì¼ë°˜ ëª¨ë¸ì˜ ì¶œë ¥ì€ ì´ë¯¸ êµ¬ì¡°í™”ëœ ë”•ì…”ë„ˆë¦¬ í˜•íƒœì„
        return {
            'generatedBasicFacts': main_output.get('reconstructed_facts', ''),
            'generatedConclusion': main_output.get('legal_conclusion', '')
        }

    # ê¸°ì¡´ ì‚¬ìš©ì ëª¨ë¸(main.py) ì¶œë ¥ ì²˜ë¦¬ ë¡œì§
    final_answer = main_output.get('final_answer', '')
    
    # ë§ˆí¬ë‹¤ìš´ íŒŒì‹±í•˜ì—¬ ê¸°ì´ˆ ì‚¬ì‹¤ê³¼ ìµœì¢… ê²°ë¡  ì¶”ì¶œ
    basic_facts_match = re.search(r'\*\*ê¸°ì´ˆ ì‚¬ì‹¤\*\*\n([\s\S]*?)(?=\n\n\*\*ë²•ì  ìŸì \*\*|$)', final_answer)
    generated_basic_facts_raw = basic_facts_match.group(1).strip() if basic_facts_match else ''
    
    # Remove markdown bullet points and ensure consistent numbering/spacing
    cleaned_facts_lines = []
    for line in generated_basic_facts_raw.split('\n'):
        cleaned_line = re.sub(r'^\*\s*(\d+\.\s*)?', r'\1', line).strip()
        if cleaned_line:
            cleaned_facts_lines.append(cleaned_line)
    generated_basic_facts = '\n'.join(cleaned_facts_lines)
    
    final_conclusion_match = re.search(r'### ğŸ§‘â€âš–ï¸ ìµœì¢… ê²°ë¡ \n\n([\s\S]*)', final_answer)
    generated_conclusion = final_conclusion_match.group(1).strip() if final_conclusion_match else ''

    return {
        'generatedBasicFacts': generated_basic_facts,
        'generatedConclusion': generated_conclusion
    }

def calculate_metrics(reference_text: str, generated_text: str) -> Dict:
    """
    ì£¼ì–´ì§„ ì°¸ì¡° í…ìŠ¤íŠ¸ì™€ ìƒì„±ëœ í…ìŠ¤íŠ¸ ê°„ì˜ BLEU, ROUGE, BertScoreë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    metrics = {}

    # Initialize Konlpy Okt for Korean tokenization
    okt = Okt()

    # Tokenize texts using Konlpy for better Korean processing
    reference_tokens = okt.morphs(reference_text)
    generated_tokens = okt.morphs(generated_text)

    # BLEU Score
    # SmoothingFunction().method1ì€ ì§§ì€ ë¬¸ì¥ì— ëŒ€í•œ BLEU ì ìˆ˜ ê³„ì‚° ì‹œ ZeroDivisionError ë°©ì§€
    if len(generated_tokens) > 0: # generated_tokensê°€ ë¹„ì–´ìˆì§€ ì•Šì„ ë•Œë§Œ ê³„ì‚°
        metrics['BLEU'] = sentence_bleu([reference_tokens], generated_tokens, smoothing_function=SmoothingFunction().method1)
    else:
        metrics['BLEU'] = 0.0

    # ROUGE Score
    rouge = Rouge()
    try:
        # ROUGEëŠ” í† í°í™”ë˜ì§€ ì•Šì€ ë¬¸ìì—´ì„ ì…ë ¥ìœ¼ë¡œ ë°›ìœ¼ë¯€ë¡œ, ë‹¤ì‹œ ë¬¸ìì—´ë¡œ ì¡°ì¸
        scores = rouge.get_scores(" ".join(generated_tokens), " ".join(reference_tokens))
        metrics['ROUGE-1_f'] = scores[0]['rouge-1']['f']
        metrics['ROUGE-2_f'] = scores[0]['rouge-2']['f']
        metrics['ROUGE-L_f'] = scores[0]['rouge-l']['f']
    except ValueError: # ì…ë ¥ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ê±°ë‚˜ ë¹„ì–´ìˆì„ ê²½ìš°
        metrics['ROUGE-1_f'] = 0.0
        metrics['ROUGE-2_f'] = 0.0
        metrics['ROUGE-L_f'] = 0.0

    # BertScore
    # BertScoreëŠ” ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    # 'bert-base-multilingual-cased'ì™€ ê°™ì€ ë‹¤êµ­ì–´ ëª¨ë¸ ì‚¬ìš© ê³ ë ¤
    try:
        P, R, F1 = bert_score_score([generated_text], [reference_text], lang="ko", verbose=False)
        metrics['BertScore_P'] = P.mean().item()
        metrics['BertScore_R'] = R.mean().item()
        metrics['BertScore_F1'] = F1.mean().item()
    except Exception as e:
        print(f"BertScore calculation error: {e}")
        metrics['BertScore_P'] = 0.0
        metrics['BertScore_R'] = 0.0
        metrics['BertScore_F1'] = 0.0

    return metrics

def main_evaluation_loop(num_evaluations: int = 1) -> List[Dict]:
    """
    í‰ê°€ ë£¨í”„ë¥¼ ì‹¤í–‰í•˜ê³  ê° í‰ê°€ì˜ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    all_results = []
    for i in range(num_evaluations):
        print(f"--- Evaluation {i+1}/{num_evaluations} ---")
        try:
            # 1. íŒë¡€ ë°ì´í„° ë¡œë“œ
            precedent_file = select_random_precedent_file()
            print(f"Selected precedent: {os.path.basename(precedent_file)}")
            precedent_data = load_precedent_data(precedent_file)
            
            reference_basic_facts = "\\n".join(precedent_data['basicFacts'])
            reference_conclusion = precedent_data['referenceConclusion']

            # 2. í´ë¼ì´ì–¸íŠ¸ ìš”ì²­ í…ìŠ¤íŠ¸ ìƒì„±
            client_request = generate_client_request_text(precedent_data['basicFacts'])

            # 3. ì‚¬ìš©ì ëª¨ë¸ ì‹¤í–‰ ë° ê²°ê³¼ ì¶”ì¶œ
            user_model_output = run_main_and_get_output(client_request)
            user_generated_texts = extract_generated_texts(user_model_output, is_general_model=False)
            
            # 4. ì¼ë°˜ ëª¨ë¸ ì‹¤í–‰ ë° ê²°ê³¼ ì¶”ì¶œ
            general_model_output = run_general_model_and_get_output(client_request)
            general_generated_texts = extract_generated_texts(general_model_output, is_general_model=True)

            # 5. ì§€í‘œ ê³„ì‚°
            user_facts_metrics = calculate_metrics(reference_basic_facts, user_generated_texts['generatedBasicFacts'])
            user_conclusion_metrics = calculate_metrics(reference_conclusion, user_generated_texts['generatedConclusion'])
            
            general_facts_metrics = calculate_metrics(reference_basic_facts, general_generated_texts['generatedBasicFacts'])
            general_conclusion_metrics = calculate_metrics(reference_conclusion, general_generated_texts['generatedConclusion'])

            all_results.append({
                'precedent_file': os.path.basename(precedent_file),
                'user_model': {
                    'facts_metrics': user_facts_metrics,
                    'conclusion_metrics': user_conclusion_metrics
                },
                'general_model': {
                    'facts_metrics': general_facts_metrics,
                    'conclusion_metrics': general_conclusion_metrics
                }
            })
        except Exception as e:
            print(f"Error during evaluation {i+1}: {e}")
            continue
    return all_results

def display_results(results: List[Dict]):
    """
    í‰ê°€ ê²°ê³¼ë¥¼ ë³´ê¸° ì‰¬ìš´ í˜•íƒœë¡œ ì¬êµ¬ì„±í•˜ì—¬ ì¶œë ¥í•©ë‹ˆë‹¤.
    """
    if not results:
        print("No evaluation results to display.")
        return

    print("\n--- Evaluation Results ---")

    for i, res in enumerate(results):
        print(f"\n--- Result for Precedent {i+1}: {res['precedent_file']} ---")

        # --- Facts Metrics Table ---
        print("\n[1. Facts Metrics]")
        facts_header = ["Metric", "User Model", "General Model"]
        print(f"| {' | '.join(facts_header)} |")
        print(f"|{'|'.join(['---' * len(h) for h in facts_header])}|")
        
        facts_metrics = [
            ("BLEU", res['user_model']['facts_metrics'].get('BLEU', 0.0), res['general_model']['facts_metrics'].get('BLEU', 0.0)),
            ("ROUGE-L", res['user_model']['facts_metrics'].get('ROUGE-L_f', 0.0), res['general_model']['facts_metrics'].get('ROUGE-L_f', 0.0)),
            ("BertScore-F1", res['user_model']['facts_metrics'].get('BertScore_F1', 0.0), res['general_model']['facts_metrics'].get('BertScore_F1', 0.0))
        ]

        for metric_name, user_score, general_score in facts_metrics:
            print(f"| {metric_name:<12} | {user_score:<10.4f} | {general_score:<13.4f} |")

        # --- Conclusion Metrics Table ---
        print("\n[2. Conclusion Metrics]")
        conclusion_header = ["Metric", "User Model", "General Model"]
        print(f"| {' | '.join(conclusion_header)} |")
        print(f"|{'|'.join(['---' * len(h) for h in conclusion_header])}|")

        conclusion_metrics = [
            ("BLEU", res['user_model']['conclusion_metrics'].get('BLEU', 0.0), res['general_model']['conclusion_metrics'].get('BLEU', 0.0)),
            ("ROUGE-L", res['user_model']['conclusion_metrics'].get('ROUGE-L_f', 0.0), res['general_model']['conclusion_metrics'].get('ROUGE-L_f', 0.0)),
            ("BertScore-F1", res['user_model']['conclusion_metrics'].get('BertScore_F1', 0.0), res['general_model']['conclusion_metrics'].get('BertScore_F1', 0.0))
        ]

        for metric_name, user_score, general_score in conclusion_metrics:
            print(f"| {metric_name:<12} | {user_score:<10.4f} | {general_score:<13.4f} |")


if __name__ == "__main__":
    # NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ (ìµœì´ˆ 1íšŒ ì‹¤í–‰ í•„ìš”)
    try:
        import nltk
        nltk.data.find('tokenizers/punkt')
    except nltk.downloader.Downloader:
        nltk.download('punkt')
    
    # í‰ê°€ ì‹¤í–‰ (ì˜ˆ: 3ê°œì˜ íŒë¡€ì— ëŒ€í•´ í‰ê°€)
    evaluation_results = main_evaluation_loop(num_evaluations=1) # ì¼ë‹¨ 1ê°œë¡œ í…ŒìŠ¤íŠ¸
    display_results(evaluation_results)
